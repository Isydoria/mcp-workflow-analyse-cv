"""
Workflow Execution Logic
Generated by LightOn Workflow Builder
"""

import logging
import os
from typing import Any, Dict, List
from dotenv import load_dotenv
from paradigm_client import ParadigmClient

# Load environment variables from .env file
load_dotenv()

logger = logging.getLogger(__name__)


class WorkflowExecutor:
    """
    Executes the workflow using the Paradigm API.
    Supports multiple input modes:
    - File paths (for Claude Desktop with local files)
    - File IDs (for direct Paradigm document IDs)
    - Paradigm context (future - documents from Paradigm workspace)
    - Legacy attached_file_ids (from Workflow Builder web interface)
    """

    def __init__(self, paradigm_client: ParadigmClient):
        self.paradigm_client = paradigm_client

    async def upload_file_from_path(self, file_path: str) -> int:
        """
        Upload a file from local filesystem to Paradigm.

        Args:
            file_path: Path to the file to upload

        Returns:
            int: File ID in Paradigm

        Raises:
            FileNotFoundError: If file doesn't exist
            Exception: If upload fails
        """
        if not os.path.exists(file_path):
            raise FileNotFoundError("File not found: {}".format(file_path))

        logger.info("Uploading file: {}".format(file_path))

        # Read file content
        with open(file_path, 'rb') as f:
            file_content = f.read()

        filename = os.path.basename(file_path)

        # Upload to Paradigm
        result = await self.paradigm_client.upload_file(
            file_content=file_content,
            filename=filename,
            collection_type='private'
        )

        file_id = result.get('id') or result.get('file_id')
        logger.info("File uploaded successfully: {} (ID: {})".format(filename, file_id))

        # Wait for file to be embedded/indexed
        logger.info("Waiting for file {} to be indexed...".format(file_id))
        await self.paradigm_client.wait_for_embedding(
            file_id=file_id,
            max_wait_time=300,
            poll_interval=2
        )
        logger.info("File {} is ready".format(file_id))

        return file_id

    async def process_file_inputs(self, **kwargs) -> List[int]:
        """
        Process file inputs from various sources and return file IDs.

        Supports:
        - file_paths: List of local file paths to upload
        - cv_file_path, job_posting_file_path, etc.: Individual file paths
        - file_ids: Direct Paradigm document IDs
        - paradigm_context: Future Paradigm MCP context
        - Legacy attached_file_ids

        Returns:
            List[int]: List of file IDs ready to use
        """
        file_ids = []

        # MODE 1: File paths provided (Claude Desktop with local files)
        if 'file_paths' in kwargs and kwargs['file_paths']:
            logger.info("MODE 1: Uploading files from paths")
            for path in kwargs['file_paths']:
                file_id = await self.upload_file_from_path(path)
                file_ids.append(file_id)

        # MODE 1b: Individual file path parameters (cv_file_path, job_posting_file_path, etc.)
        elif any(k.endswith('_file_path') for k in kwargs.keys()):
            logger.info("MODE 1b: Uploading individual files from paths")
            for key, value in kwargs.items():
                if key.endswith('_file_path') and value:
                    file_id = await self.upload_file_from_path(value)
                    file_ids.append(file_id)

        # MODE 2: File IDs provided directly
        elif 'file_ids' in kwargs and kwargs['file_ids']:
            logger.info("MODE 2: Using provided file IDs")
            file_ids = [int(fid) for fid in kwargs['file_ids']]

        # MODE 3: Paradigm context (future integration)
        elif 'paradigm_context' in kwargs:
            logger.info("MODE 3: Using Paradigm context")
            context = kwargs['paradigm_context']
            file_ids = context.get('document_ids', [])

        # MODE 4: Legacy attached_file_ids (Workflow Builder web)
        else:
            logger.info("MODE 4: Using legacy attached_file_ids")
            import builtins
            if 'attached_file_ids' in globals() and globals()['attached_file_ids']:
                file_ids = globals()['attached_file_ids']
            elif hasattr(builtins, 'attached_file_ids') and builtins.attached_file_ids:
                file_ids = builtins.attached_file_ids
            else:
                logger.warning("No file inputs found")
                file_ids = []

        logger.info("Processed file inputs: {} file(s)".format(len(file_ids)))
        return file_ids

    async def execute(self, **kwargs) -> Dict[str, Any]:
        """
        Execute the workflow with given parameters.

        Automatically detects input mode and processes files accordingly.

        Args:
            **kwargs: Workflow parameters (file_paths, file_ids, or other parameters)

        Returns:
            Dict containing workflow results
        """
        try:
            # Process file inputs
            file_ids = await self.process_file_inputs(**kwargs)

            # Set attached_file_ids for legacy workflow code
            import builtins
            builtins.attached_file_ids = file_ids

            # Execute the workflow with user_input parameter
            # Extract query/user_input from kwargs, default to empty string
            user_input = kwargs.get('query', kwargs.get('user_input', ''))
            result = await execute_workflow(user_input)

            # Return structured result with file information
            return {
                "status": "success",
                "result": result,
                "files_processed": len(file_ids)
            }

        except Exception as e:
            logger.error("Workflow execution failed: {}".format(str(e)))
            raise


# Generated workflow code
import asyncio
import aiohttp
import json
import logging
import os
from typing import Optional, List, Dict, Any
import re

LIGHTON_API_KEY = os.getenv("PARADIGM_API_KEY", "your_api_key_here")
LIGHTON_BASE_URL = os.getenv("PARADIGM_BASE_URL", "https://paradigm.lighton.ai")

logger = logging.getLogger(__name__)

class ParadigmClient:
    def __init__(self, api_key: str, base_url: str = "https://paradigm.lighton.ai"):
        self.api_key = api_key
        self.base_url = base_url
        self.headers = {
            "Content-Type": "application/json",
            "Authorization": "Bearer {}".format(self.api_key)
        }
        self._session: Optional[aiohttp.ClientSession] = None
        logger.info("‚úÖ ParadigmClient initialized: {}".format(base_url))

    async def _get_session(self) -> aiohttp.ClientSession:
        if self._session is None or self._session.closed:
            self._session = aiohttp.ClientSession()
            logger.debug("üîå Created new aiohttp session")
        return self._session

    async def close(self):
        if self._session and not self._session.closed:
            await self._session.close()
            logger.debug("üîå Closed aiohttp session")
            self._session = None

    async def get_file(self, file_id: int, include_content: bool = False) -> Dict[str, Any]:
        endpoint = "{}/api/v2/files/{}".format(self.base_url, file_id)
        params = {}
        if include_content:
            params["include_content"] = "true"

        try:
            logger.info("üìÑ Getting file info for ID {}".format(file_id))
            session = await self._get_session()
            async with session.get(endpoint, params=params, headers=self.headers) as response:
                if response.status == 200:
                    result = await response.json()
                    status = result.get('status', 'unknown')
                    filename = result.get('filename', 'N/A')
                    logger.info("‚úÖ File {} ({}): status={}".format(file_id, filename, status))
                    return result
                elif response.status == 404:
                    error_text = await response.text()
                    logger.error("‚ùå File {} not found".format(file_id))
                    raise Exception("File {} not found: {}".format(file_id, error_text))
                else:
                    error_text = await response.text()
                    logger.error("‚ùå Get file failed: {}".format(response.status))
                    raise Exception("Get file API error {}: {}".format(response.status, error_text))
        except Exception as e:
            logger.error("‚ùå Get file error: {}".format(str(e)))
            raise

    async def wait_for_embedding(self, file_id: int, max_wait_time: int = 300, poll_interval: int = 2) -> Dict[str, Any]:
        try:
            logger.info("‚è≥ Waiting for file {} to be embedded (max={}s, interval={}s)".format(file_id, max_wait_time, poll_interval))
            elapsed = 0
            while elapsed < max_wait_time:
                file_info = await self.get_file(file_id)
                status = file_info.get('status', '').lower()
                filename = file_info.get('filename', 'N/A')

                logger.info("üîÑ File {} ({}): status={} (elapsed: {}s)".format(file_id, filename, status, elapsed))

                if status == 'embedded':
                    logger.info("‚úÖ File {} is embedded and ready!".format(file_id))
                    return file_info
                elif status == 'failed':
                    logger.error("‚ùå File {} embedding failed".format(file_id))
                    raise Exception("File {} embedding failed".format(file_id))

                await asyncio.sleep(poll_interval)
                elapsed += poll_interval

            logger.error("‚è∞ Timeout waiting for file {} after {}s".format(file_id, max_wait_time))
            raise Exception("Timeout waiting for file {} to be embedded".format(file_id))
        except Exception as e:
            logger.error("‚ùå Wait for embedding error: {}".format(str(e)))
            raise

    async def analyze_documents_with_polling(self, query: str, document_ids: List[int], model: Optional[str] = None, private: bool = True, max_wait_time: int = 300, poll_interval: int = 5) -> str:
        try:
            logger.info("üìä Analysis with polling: max={}s, interval={}s".format(max_wait_time, poll_interval))
            chat_response_id = await self.document_analysis_start(query, document_ids, model, private)
            elapsed = 0
            while elapsed < max_wait_time:
                try:
                    result = await self.document_analysis_get_result(chat_response_id)
                    status = result.get("status", "").lower()
                    logger.info("üîÑ Polling: {} (elapsed: {}s)".format(status, elapsed))

                    if status in ["completed", "complete", "finished", "success"]:
                        analysis_result = result.get("result") or result.get("detailed_analysis")
                        if analysis_result:
                            logger.info("‚úÖ Analysis done! ({} chars)".format(len(analysis_result)))
                            return analysis_result
                        else:
                            return "Analysis completed but no result was returned"
                    elif status in ["failed", "error"]:
                        logger.error("‚ùå Analysis failed: {}".format(status))
                        raise Exception("Analysis failed with status: {}".format(status))

                    await asyncio.sleep(poll_interval)
                    elapsed += poll_interval
                except Exception as e:
                    if "not found" in str(e).lower() or "404" in str(e):
                        logger.info("‚è≥ Still running... ({}s)".format(elapsed))
                        await asyncio.sleep(poll_interval)
                        elapsed += poll_interval
                        continue
                    else:
                        raise

            logger.error("‚è∞ Timeout after {}s".format(max_wait_time))
            raise Exception("Analysis timed out after {} seconds".format(max_wait_time))
        except Exception as e:
            logger.error("‚ùå Analysis with polling failed: {}".format(str(e)))
            return "Document analysis failed: {}".format(str(e))

    async def document_analysis_start(self, query: str, document_ids: List[int], model: Optional[str] = None, private: bool = True) -> str:
        endpoint = "{}/api/v2/chat/document-analysis".format(self.base_url)
        payload = {"query": query, "document_ids": document_ids}
        if model:
            payload["model"] = model

        try:
            logger.info("üìä Starting analysis: {}...".format(query[:50]))
            session = await self._get_session()
            async with session.post(endpoint, json=payload, headers=self.headers) as response:
                if response.status == 200:
                    result = await response.json()
                    chat_response_id = result.get("chat_response_id")
                    logger.info("‚úÖ Analysis started: {}".format(chat_response_id))
                    return chat_response_id
                else:
                    error_text = await response.text()
                    logger.error("‚ùå Analysis start failed: {}".format(response.status))
                    raise Exception("Failed to start analysis: {} - {}".format(response.status, error_text))
        except Exception as e:
            logger.error("‚ùå Analysis start error: {}".format(str(e)))
            raise

    async def document_analysis_get_result(self, chat_response_id: str) -> Dict[str, Any]:
        endpoint = "{}/api/v2/chat/document-analysis/{}".format(self.base_url, chat_response_id)
        try:
            session = await self._get_session()
            async with session.get(endpoint, headers=self.headers) as response:
                if response.status == 200:
                    return await response.json()
                elif response.status == 404:
                    return {"status": "processing"}
                else:
                    error_text = await response.text()
                    raise Exception("Failed to get analysis result: {}".format(response.status))
        except Exception as e:
            logger.error("‚ùå Get result error: {}".format(str(e)))
            raise

    async def document_search(self, query: str, file_ids: Optional[List[int]] = None, workspace_ids: Optional[List[int]] = None, chat_session_id: Optional[str] = None, model: Optional[str] = None, company_scope: bool = False, private_scope: bool = True, tool: str = "DocumentSearch", private: bool = True) -> Dict[str, Any]:
        endpoint = "{}/api/v2/chat/document-search".format(self.base_url)
        payload = {"query": query, "company_scope": company_scope, "private_scope": private_scope, "tool": tool, "private": private}

        if file_ids:
            payload["file_ids"] = file_ids
        if workspace_ids:
            payload["workspace_ids"] = workspace_ids
        if chat_session_id:
            payload["chat_session_id"] = chat_session_id
        if model:
            payload["model"] = model

        try:
            logger.info("üîç Document Search: {}... (tool={})".format(query[:50], tool))
            session = await self._get_session()
            async with session.post(endpoint, json=payload, headers=self.headers) as response:
                if response.status == 200:
                    result = await response.json()
                    logger.info("‚úÖ Search completed: {} documents".format(len(result.get('documents', []))))
                    return result
                else:
                    error_text = await response.text()
                    logger.error("‚ùå Search failed: {} - {}".format(response.status, error_text))
                    raise Exception("Document search failed: {} - {}".format(response.status, error_text))
        except Exception as e:
            logger.error("‚ùå Search error: {}".format(str(e)))
            raise

    async def chat_completion(self, prompt: str, model: str = "alfred-sv5", system_prompt: Optional[str] = None, guided_choice: Optional[List[str]] = None, guided_json: Optional[Dict[str, Any]] = None, guided_regex: Optional[str] = None) -> str:
        endpoint = "{}/api/v2/chat/completions".format(self.base_url)
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})

        payload = {"model": model, "messages": messages}
        if guided_choice:
            payload["guided_choice"] = guided_choice
        if guided_json:
            payload["guided_json"] = guided_json
        if guided_regex:
            payload["guided_regex"] = guided_regex

        try:
            logger.info("üí¨ Chat completion: {}...".format(prompt[:50]))
            session = await self._get_session()
            async with session.post(endpoint, json=payload, headers=self.headers) as response:
                if response.status == 200:
                    result = await response.json()
                    answer = result["choices"][0]["message"]["content"]
                    logger.info("‚úÖ Chat completed ({} chars)".format(len(answer)))
                    return answer
                else:
                    error_text = await response.text()
                    logger.error("‚ùå Chat failed: {}".format(response.status))
                    raise Exception("Chat completion failed: {}".format(response.status))
        except Exception as e:
            logger.error("‚ùå Chat error: {}".format(str(e)))
            raise

    async def upload_file(self, file_content: bytes, filename: str, collection_type: str = "private") -> Dict[str, Any]:
        endpoint = "{}/api/v2/files".format(self.base_url)
        data = aiohttp.FormData()
        data.add_field('file', file_content, filename=filename)
        data.add_field('collection_type', collection_type)
        headers = {"Authorization": "Bearer {}".format(self.api_key)}

        try:
            logger.info("üìÅ Uploading: {} ({} bytes)".format(filename, len(file_content)))
            session = await self._get_session()
            async with session.post(endpoint, data=data, headers=headers) as response:
                if response.status in [200, 201]:
                    result = await response.json()
                    file_id = result.get("id") or result.get("file_id")
                    logger.info("‚úÖ File uploaded: ID={}".format(file_id))
                    return result
                else:
                    error_text = await response.text()
                    logger.error("‚ùå Upload failed: {}".format(response.status))
                    raise Exception("File upload failed: {}".format(response.status))
        except Exception as e:
            logger.error("‚ùå Upload error: {}".format(str(e)))
            raise

paradigm_client = ParadigmClient(LIGHTON_API_KEY, LIGHTON_BASE_URL)

async def execute_workflow(user_input: str) -> str:
    try:
        import builtins
        attached_files = None
        if 'attached_file_ids' in globals() and globals()['attached_file_ids']:
            attached_files = globals()['attached_file_ids']
        elif hasattr(builtins, 'attached_file_ids') and builtins.attached_file_ids:
            attached_files = builtins.attached_file_ids

        if not attached_files or len(attached_files) < 6:
            return "‚ùå ERREUR: Ce workflow n√©cessite exactement 6 fichiers PDF (1 fiche de poste + 5 CV). Veuillez uploader tous les fichiers n√©cessaires."

        file_ids = [int(f) for f in attached_files[:6]]
        logger.info("üöÄ D√©marrage analyse CV: 6 fichiers d√©tect√©s")

        # √âTAPE 1: Attendre l'indexation compl√®te de tous les fichiers
        logger.info("‚è≥ √âTAPE 1: V√©rification indexation des 6 fichiers...")
        failed_files = []
        
        for i, file_id in enumerate(file_ids):
            try:
                logger.info("üîÑ V√©rification fichier {} ({}/{})".format(file_id, i+1, len(file_ids)))
                file_info = await paradigm_client.wait_for_embedding(
                    file_id=file_id,
                    max_wait_time=120,
                    poll_interval=2
                )
                logger.info("‚úÖ Fichier {} index√©: {}".format(file_id, file_info.get('filename', 'N/A')))
            except Exception as e:
                logger.error("‚ùå √âchec indexation fichier {}: {}".format(file_id, str(e)))
                failed_files.append(file_id)
                
                logger.info("‚è≥ Attente suppl√©mentaire de 90s pour fichier {}...".format(file_id))
                await asyncio.sleep(90)
                
                try:
                    file_info = await paradigm_client.get_file(file_id)
                    if file_info.get('status', '').lower() == 'embedded':
                        logger.info("‚úÖ Fichier {} finalement pr√™t apr√®s attente suppl√©mentaire".format(file_id))
                        failed_files.remove(file_id)
                    else:
                        logger.error("‚ùå Fichier {} toujours non index√©: status={}".format(file_id, file_info.get('status')))
                except:
                    logger.error("‚ùå Fichier {} d√©finitivement non accessible".format(file_id))

        if failed_files:
            return "‚ùå ERREUR: Fichiers non index√©s: {}. L'analyse ne peut pas continuer. Veuillez r√©essayer avec des fichiers valides.".format(failed_files)

        logger.info("‚úÖ √âTAPE 1 termin√©e: Tous les fichiers sont index√©s et pr√™ts")

        # √âTAPE 2: Extraire le contenu de la fiche de poste (premier fichier)
        logger.info("üìä √âTAPE 2: Extraction fiche de poste...")
        fiche_poste_id = file_ids[0]
        
        try:
            fiche_poste_content = await paradigm_client.analyze_documents_with_polling(
                query="Extraire toutes les informations de cette fiche de poste : missions principales, comp√©tences techniques requises, comp√©tences comportementales souhait√©es, exp√©rience minimale requise, formation attendue, langues n√©cessaires, et tous autres crit√®res de s√©lection mentionn√©s",
                document_ids=[fiche_poste_id],
                max_wait_time=180,
                poll_interval=3
            )
            
            if len(fiche_poste_content) < 500:
                logger.warning("‚ö†Ô∏è Extraction fiche de poste insuffisante, fallback VisionDocumentSearch...")
                fallback_result = await paradigm_client.document_search(
                    query="Extraire toutes les informations de cette fiche de poste compl√®te",
                    file_ids=[fiche_poste_id],
                    tool="VisionDocumentSearch",
                    company_scope=False,
                    private_scope=False
                )
                fiche_poste_content = fallback_result.get('answer', fiche_poste_content)
                
        except Exception as e:
            logger.error("‚ùå √âchec extraction fiche de poste: {}".format(str(e)))
            return "‚ùå ERREUR: Impossible d'extraire la fiche de poste. V√©rifiez que le premier fichier est une fiche de poste valide."

        logger.info("‚úÖ √âTAPE 2 termin√©e: Fiche de poste extraite ({} chars)".format(len(fiche_poste_content)))

        # √âTAPES 3a-3e: Extraire les informations des 5 CV en parall√®le
        logger.info("üîÑ √âTAPES 3a-3e: Extraction CV des 5 candidats en parall√®le...")
        cv_ids = file_ids[1:6]  # Les 5 derniers fichiers sont les CV
        
        # Pr√©parer les t√¢ches d'extraction en parall√®le
        cv_extraction_tasks = []
        for i, cv_id in enumerate(cv_ids):
            task = extract_cv_with_fallback(cv_id, i+1)
            cv_extraction_tasks.append(task)
        
        # Ex√©cuter toutes les extractions en parall√®le avec gestion des erreurs
        cv_extractions = []
        try:
            cv_extractions = await asyncio.gather(*cv_extraction_tasks, return_exceptions=True)
        except Exception as e:
            logger.error("‚ùå Erreur dans extractions parall√®les: {}".format(str(e)))
            return "‚ùå ERREUR: √âchec lors de l'extraction des CV. V√©rifiez que tous les fichiers sont des CV valides."

        # V√©rifier les r√©sultats d'extraction
        valid_extractions = []
        for i, result in enumerate(cv_extractions):
            if isinstance(result, Exception):
                logger.error("‚ùå Extraction CV {} √©chou√©e: {}".format(i+1, str(result)))
                valid_extractions.append("EXTRACTION_FAILED")
            else:
                valid_extractions.append(result)
                logger.info("‚úÖ CV {} extrait ({} chars)".format(i+1, len(result)))

        logger.info("‚úÖ √âTAPES 3a-3e termin√©es: {} CV extraits avec succ√®s".format(len([r for r in valid_extractions if r != "EXTRACTION_FAILED"])))

        # √âTAPE 4: Compiler et nettoyer les r√©sultats d'extraction
        logger.info("üßπ √âTAPE 4: Compilation et nettoyage des extractions...")
        
        cleaned_cv_data = []
        for i, extraction in enumerate(valid_extractions):
            if extraction == "EXTRACTION_FAILED":
                cleaned_cv_data.append({
                    'id': 'Candidat {}'.format(i+1),
                    'content': 'EXTRACTION √âCHOU√âE - Impossible d\'analyser ce CV',
                    'valid': False
                })
            elif len(extraction) < 200:
                cleaned_cv_data.append({
                    'id': 'Candidat {}'.format(i+1),
                    'content': 'EXTRACTION INSUFFISANTE - Donn√©es partielles: {}'.format(extraction[:100]),
                    'valid': False
                })
            else:
                # Nettoyage basique du contenu
                cleaned_content = clean_extraction_content(extraction, i+1)
                cleaned_cv_data.append({
                    'id': 'Candidat {}'.format(i+1),
                    'content': cleaned_content,
                    'valid': True
                })

        valid_candidates = [cv for cv in cleaned_cv_data if cv['valid']]
        logger.info("‚úÖ √âTAPE 4 termin√©e: {} candidats valides pr√©par√©s pour analyse".format(len(valid_candidates)))

        if not valid_candidates:
            return "‚ùå ERREUR: Aucun CV n'a pu √™tre extrait correctement. V√©rifiez la qualit√© des fichiers upload√©s."

        # √âTAPES 5a-5e: Analyser et scorer les candidats en parall√®le
        logger.info("üìä √âTAPES 5a-5e: √âvaluation des candidats en parall√®le...")
        
        scoring_tasks = []
        for cv_data in cleaned_cv_data:
            if cv_data['valid']:
                task = score_candidate(fiche_poste_content, cv_data['content'], cv_data['id'])
                scoring_tasks.append((cv_data['id'], task))
            else:
                scoring_tasks.append((cv_data['id'], None))

        # Ex√©cuter les √©valuations en parall√®le
        scoring_results = []
        active_tasks = [(cv_id, task) for cv_id, task in scoring_tasks if task is not None]
        
        if active_tasks:
            try:
                task_results = await asyncio.gather(*[task for cv_id, task in active_tasks], return_exceptions=True)
                
                result_index = 0
                for cv_id, task in scoring_tasks:
                    if task is None:
                        scoring_results.append((cv_id, "√âVALUATION IMPOSSIBLE - CV non extrait"))
                    else:
                        if result_index < len(task_results):
                            result = task_results[result_index]
                            if isinstance(result, Exception):
                                scoring_results.append((cv_id, "ERREUR √âVALUATION: {}".format(str(result))))
                            else:
                                scoring_results.append((cv_id, result))
                            result_index += 1
                        else:
                            scoring_results.append((cv_id, "ERREUR √âVALUATION: R√©sultat manquant"))
                            
            except Exception as e:
                logger.error("‚ùå Erreur √©valuations parall√®les: {}".format(str(e)))
                return "‚ùå ERREUR: √âchec lors de l'√©valuation des candidats."

        logger.info("‚úÖ √âTAPES 5a-5e termin√©es: {} √©valuations compl√©t√©es".format(len(scoring_results)))

        # √âTAPE 6: Compiler et nettoyer les r√©sultats d'√©valuation
        logger.info("üßπ √âTAPE 6: Compilation des r√©sultats d'√©valuation...")
        
        compiled_evaluations = []
        for cv_id, evaluation in scoring_results:
            if "ERREUR" in evaluation or "IMPOSSIBLE" in evaluation:
                compiled_evaluations.append({
                    'candidate': cv_id,
                    'evaluation': evaluation,
                    'score': 0.0,
                    'valid': False
                })
            else:
                cleaned_eval = clean_evaluation_content(evaluation, cv_id)
                score = extract_score_from_evaluation(cleaned_eval)
                compiled_evaluations.append({
                    'candidate': cv_id,
                    'evaluation': cleaned_eval,
                    'score': score,
                    'valid': True
                })

        valid_evaluations = [e for e in compiled_evaluations if e['valid']]
        logger.info("‚úÖ √âTAPE 6 termin√©e: {} √©valuations valides compil√©es".format(len(valid_evaluations)))

        # √âTAPE 7: Calculer statistiques et recommendations
        logger.info("üìà √âTAPE 7: Calcul des statistiques globales...")
        
        if valid_evaluations:
            scores = [e['score'] for e in valid_evaluations]
            avg_score = round(sum(scores) / len(scores), 1)
            
            recommendations = {
                'recommand√©': [],
                'en_r√©serve': [],
                'non_retenu': []
            }
            
            for eval_data in valid_evaluations:
                score = eval_data['score']
                candidate = eval_data['candidate']
                if score >= 70.0:
                    recommendations['recommand√©'].append(candidate)
                elif score >= 50.0:
                    recommendations['en_r√©serve'].append(candidate)
                else:
                    recommendations['non_retenu'].append(candidate)
        else:
            avg_score = 0.0
            recommendations = {'recommand√©': [], 'en_r√©serve': [], 'non_retenu': []}

        from datetime import datetime
        timestamp = datetime.now().strftime("%d/%m/%Y √† %H:%M:%S")
        
        stats = {
            'avg_score': avg_score,
            'recommendations': recommendations,
            'timestamp': timestamp,
            'total_candidates': len(compiled_evaluations),
            'valid_candidates': len(valid_evaluations)
        }
        
        logger.info("‚úÖ √âTAPE 7 termin√©e: Statistiques calcul√©es (score moyen: {}%)".format(avg_score))

        # √âTAPE 8: G√©n√©rer le rapport final
        logger.info("üìù √âTAPE 8: G√©n√©ration du rapport final...")
        
        try:
            final_report = await generate_final_report(
                fiche_poste_content, 
                compiled_evaluations, 
                stats
            )
            
            logger.info("‚úÖ √âTAPE 8 termin√©e: Rapport final g√©n√©r√© ({} chars)".format(len(final_report)))
            return final_report
            
        except Exception as e:
            logger.error("‚ùå Erreur g√©n√©ration rapport: {}".format(str(e)))
            return "‚ùå ERREUR: Impossible de g√©n√©rer le rapport final. Donn√©es partielles disponibles mais formatage √©chou√©."

    except Exception as e:
        logger.error("‚ùå Erreur workflow: {}".format(str(e)))
        return "‚ùå ERREUR WORKFLOW: {}".format(str(e))
    finally:
        await paradigm_client.close()

async def extract_cv_with_fallback(cv_id: int, candidate_num: int) -> str:
    """Extrait un CV avec fallback VisionDocumentSearch si n√©cessaire"""
    try:
        logger.info("üìÑ Extraction CV candidat {} (fichier {})".format(candidate_num, cv_id))
        
        cv_content = await paradigm_client.analyze_documents_with_polling(
            query="Extraire toutes les informations de ce CV : nom complet, email, t√©l√©phone, formation d√©taill√©e avec dipl√¥mes et √©tablissements, exp√©rience professionnelle compl√®te avec postes occup√©s et dur√©es, comp√©tences techniques incluant langages de programmation et outils, comp√©tences comportementales, langues parl√©es avec niveaux, calcul du nombre total d'ann√©es d'exp√©rience depuis le premier emploi",
            document_ids=[cv_id],
            max_wait_time=180,
            poll_interval=3
        )
        
        if len(cv_content) < 500:
            logger.warning("‚ö†Ô∏è Extraction CV {} insuffisante, fallback VisionDocumentSearch...".format(candidate_num))
            
            fallback_result = await paradigm_client.document_search(
                query="Extraire toutes les informations compl√®tes de ce CV",
                file_ids=[cv_id],
                tool="VisionDocumentSearch",
                company_scope=False,
                private_scope=False
            )
            cv_content = fallback_result.get('answer', cv_content)
            
        logger.info("‚úÖ CV candidat {} extrait ({} chars)".format(candidate_num, len(cv_content)))
        return cv_content
        
    except Exception as e:
        logger.error("‚ùå Extraction CV candidat {} √©chou√©e: {}".format(candidate_num, str(e)))
        raise

async def score_candidate(fiche_poste: str, cv_content: str, candidate_id: str) -> str:
    """Score un candidat par rapport √† la fiche de poste"""
    try:
        logger.info("üéØ √âvaluation {}...".format(candidate_id))
        
        evaluation_prompt = '''√âvaluez ce candidat par rapport √† la fiche de poste selon ces 6 crit√®res avec pond√©rations exactes :

FICHE DE POSTE :
{}

CV DU CANDIDAT :
{}

CRIT√àRES D'√âVALUATION (notez chaque crit√®re sur 100 points) :
1. Comp√©tences techniques (35%) : Ma√Ætrise des technologies, outils et m√©thodes demand√©s
2. Exp√©rience professionnelle (30%) : Pertinence et dur√©e de l'exp√©rience par rapport au poste
3. Formation (15%) : Ad√©quation du niveau et domaine de formation
4. Comp√©tences comportementales (10%) : Soft skills et aptitudes relationnelles
5. Langues (5%) : Niveau dans les langues requises
6. √âl√©ments bonus (5%) : Certifications, projets personnels, sp√©cialisations

Pour CHAQUE crit√®re, donnez :
- Score de 0 √† 100
- Justification de 2-3 phrases expliquant le score

Puis listez :
- Points forts (3-5 √©l√©ments)
- Points faibles (2-4 √©l√©ments)

Enfin, calculez le score global pond√©r√© et affichez-le clairement.'''.format(fiche_poste[:2000], cv_content[:3000])

        evaluation = await paradigm_client.chat_completion(
            prompt=evaluation_prompt,
            system_prompt='''Tu es un expert en recrutement qui √©value des candidats.

üåç R√àGLES DE LANGUE (CRITIQUE):
- R√©ponds UNIQUEMENT en FRAN√áAIS
- N'utilise AUCUN mot d'une autre langue

üìù R√àGLES DE FORMATAGE:
- Utilise Markdown propre : ## pour titres, **gras** pour scores
- INTERDICTION ABSOLUE de balises entre crochets comme [ATTENTION], [SCORE], [ANALYSE]
- √âcris "Points forts :" et NON "Points forts [AVANTAGES] :"
- Pas de pr√©ambule, √©cris directement le contenu

üë§ R√àGLES POUR LES NOMS:
- TOUJOURS afficher les NOMS COMPLETS (Pr√©nom NOM)
- NE JAMAIS utiliser uniquement pr√©noms ou identifiants'''
        )
        
        logger.info("‚úÖ √âvaluation {} termin√©e ({} chars)".format(candidate_id, len(evaluation)))
        return evaluation
        
    except Exception as e:
        logger.error("‚ùå Erreur √©valuation {}: {}".format(candidate_id, str(e)))
        raise

def clean_extraction_content(content: str, candidate_num: int) -> str:
    """Nettoie le contenu d'extraction d'un CV"""
    # Supprime les m√©tadonn√©es et commentaires d'IA
    cleaned = re.sub(r'(?i)(voici|here is|selon|based on).*?:', '', content)
    cleaned = re.sub(r'\*\*.*?\*\*:', '', cleaned)  # Remove bold headers
    cleaned = re.sub(r'^\s*-\s*', '', cleaned, flags=re.MULTILINE)
    
    # Standardise le format des noms
    name_pattern = r'(?i)nom\s*:?\s*([A-Z√Å√â√à√ä√ã√é√è√î√ñ√ô√õ√ú≈∏√á][a-z√°√©√®√™√´√Æ√Ø√¥√∂√π√ª√º√ø√ß]+(?:\s+[A-Z√Å√â√à√ä√ã√é√è√î√ñ√ô√õ√ú≈∏√á][a-z√°√©√®√™√´√Æ√Ø√¥√∂√π√ª√º√ø√ß]*)*)'
    name_match = re.search(name_pattern, cleaned)
    if name_match:
        name_parts = name_match.group(1).split()
        if len(name_parts) >= 2:
            formatted_name = "{} {}".format(name_parts[0].title(), " ".join(n.upper() for n in name_parts[1:]))
            cleaned = re.sub(name_pattern, "Nom : {}".format(formatted_name), cleaned, count=1)
    
    return cleaned.strip()

def clean_evaluation_content(content: str, candidate_id: str) -> str:
    """Nettoie le contenu d'√©valuation d'un candidat"""
    # Supprime les balises entre crochets
    cleaned = re.sub(r'\[.*?\]', '', content)
    
    # Supprime les m√©tadonn√©es d'IA
    cleaned = re.sub(r'(?i)(voici|here is).*?:', '', cleaned)
    
    # Uniformise les formats de scores
    cleaned = re.sub(r'(\d+)/100', r'**\1/100**', cleaned)
    cleaned = re.sub(r'Score\s*:?\s*(\d+)', r'**Score : \1/100**', cleaned)
    
    return cleaned.strip()

def extract_score_from_evaluation(evaluation: str) -> float:
    """Extrait le score final d'une √©valuation"""
    # Cherche le score global pond√©r√©
    patterns = [
        r'(?i)score\s+global.*?(\d+\.?\d*)',
        r'(?i)total.*?(\d+\.?\d*)',
        r'(?i)final.*?(\d+\.?\d*)',
        r'(?i)pond√©r√©.*?(\d+\.?\d*)',
        r'(\d+\.?\d*)\s*%\s*$',
        r'(\d+\.?\d*)/100'
    ]
    
    for pattern in patterns:
        matches = re.findall(pattern, evaluation)
        if matches:
            try:
                score = float(matches[-1])  # Prend le dernier match (plus susceptible d'√™tre le score final)
                if 0 <= score <= 100:
                    return round(score, 1)
            except ValueError:
                continue
    
    # Si aucun score trouv√©, retourne 0
    return 0.0

async def generate_final_report(fiche_poste: str, evaluations: List[Dict], stats: Dict) -> str:
    """G√©n√®re le rapport final au format Markdown"""
    
    # Pr√©pare les donn√©es pour le prompt
    evaluations_text = ""
    for eval_data in evaluations:
        evaluations_text += "=== {} ===\n".format(eval_data['candidate'])
        evaluations_text += "Score: {}/100\n".format(eval_data['score'])
        evaluations_text += "{}\n\n".format(eval_data['evaluation'][:1500])
    
    recommendations_text = ""
    for category, candidates in stats['recommendations'].items():
        if candidates:
            recommendations_text += "{}: {}\n".format(category.replace('_', ' ').title(), ', '.join(candidates))
    
    report_prompt = '''G√©n√®re un rapport professionnel d'analyse des candidatures avec ces donn√©es :

FICHE DE POSTE :
{}

√âVALUATIONS DES CANDIDATS :
{}

STATISTIQUES :
- Score moyen : {}%
- Candidats valides : {}/{}
- Analyse effectu√©e le : {}

RECOMMANDATIONS PAR SEUILS :
{}

Structure le rapport ainsi :
## üìã Fiche de Poste - R√©sum√©

## üë• Analyse des Candidats
[Pr√©sentation √©gale des 5 candidats sans classement, avec scores et justifications]

## üéØ Recommandations par Seuils
[‚â•70% = Recommand√©, 50-69% = En r√©serve, <50% = Non retenu]

## üìä Synth√®se Globale
[Statistiques et observations g√©n√©rales]

[Horodatage complet]'''.format(
        fiche_poste[:1000], 
        evaluations_text[:4000], 
        stats['avg_score'],
        stats['valid_candidates'],
        stats['total_candidates'],
        stats['timestamp'],
        recommendations_text
    )

    try:
        report = await paradigm_client.chat_completion(
            prompt=report_prompt,
            system_prompt='''Tu es un expert RH qui g√©n√®re des rapports d'analyse de candidatures.

üåç R√àGLES DE LANGUE (CRITIQUE):
- R√©ponds UNIQUEMENT en FRAN√áAIS
- N'utilise AUCUN mot d'une autre langue

üìù R√àGLES DE FORMATAGE:
- Utilise Markdown propre avec emojis : ## pour titres, **gras**, - pour listes
- INTERDICTION ABSOLUE de balises entre crochets comme [RECOMMANDATION], [ANALYSE], [SCORE]
- √âcris "Points forts :" et NON "Points forts [AVANTAGES] :"
- Pas de pr√©ambule ("Voici le rapport"), √©cris directement le contenu

üë• R√àGLES DE PR√âSENTATION:
- Pr√©sentation √âGALE de tous les candidats, PAS de hi√©rarchie ni classement
- TOUJOURS afficher les NOMS COMPLETS (Pr√©nom NOM), jamais d'abr√©viations
- Neutralit√© absolue dans la pr√©sentation entre candidats''')
        
        return report
        
    except Exception as e:
        logger.error("‚ùå Erreur g√©n√©ration rapport: {}".format(str(e)))
        
        # Fallback: g√©n√©ration basique du rapport
        fallback_report = generate_basic_report(fiche_poste, evaluations, stats)
        return fallback_report

def generate_basic_report(fiche_poste: str, evaluations: List[Dict], stats: Dict) -> str:
    """G√©n√®re un rapport basique en cas d'√©chec de l'IA"""
    
    report = "# üìã Rapport d'Analyse des Candidatures\n\n"
    
    # R√©sum√© fiche de poste
    report += "## üìã Fiche de Poste - R√©sum√©\n\n"
    report += "{}\n\n".format(fiche_poste[:500])
    
    # Analyse des candidats
    report += "## üë• Analyse des Candidats\n\n"
    for eval_data in evaluations:
        report += "### {}\n".format(eval_data['candidate'])
        report += "**Score :** {}/100\n\n".format(eval_data['score'])
        if eval_data['valid']:
            report += "{}\n\n".format(eval_data['evaluation'][:800])
        else:
            report += "‚ùå {}\n\n".format(eval_data['evaluation'])
        report += "---\n\n"
    
    # Recommandations
    report += "## üéØ Recommandations par Seuils\n\n"
    for category, candidates in stats['recommendations'].items():
        if candidates:
            emoji = "‚úÖ" if category == "recommand√©" else "‚ö†Ô∏è" if category == "en_r√©serve" else "‚ùå"
            report += "{} **{}** : {}\n\n".format(emoji, category.replace('_', ' ').title(), ', '.join(candidates))
    
    # Synth√®se
    report += "## üìä Synth√®se Globale\n\n"
    report += "- **Score moyen :** {}%\n".format(stats['avg_score'])
    report += "- **Candidats analys√©s :** {}/{}\n".format(stats['valid_candidates'], stats['total_candidates'])
    report += "- **Analyse effectu√©e le :** {}\n\n".format(stats['timestamp'])
    
    return report
